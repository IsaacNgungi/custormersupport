{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daed677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Customer support chatbot using Mistral API and FAISS\n",
    "## Define categories and keywords\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9e7c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Mistral client\n",
    "from mistralai import Mistral\n",
    "\n",
    "from dotenv import find_dotenv\n",
    "\n",
    "\n",
    "def load_environment_variables():\n",
    "    find_dotenv('Machine learning_projects/.env')\n",
    "    MISTRAL_API = os.getenv(\"MISTRAL_API\")\n",
    "    client = Mistral(api_key=MISTRAL_API)\n",
    "    return MISTRAL_API, client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d973a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the state class\n",
    "class ChatbotState:\n",
    "    def __init__(self):\n",
    "        self.category = None\n",
    "        self.user_query = None\n",
    "        self.context = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1100f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"Billing\": [\"invoice\", \"payment\", \"charged\", \"refund\"],\n",
    "    \"Technical Support\": [\"error\", \"problem\", \"not working\", \"crash\", \"issue\", \"bug\"],\n",
    "    \"General Inquiry\": [\"contact\", \"hours\", \"location\", \"address\", \"help\"]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b33c264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to categorize user queries\n",
    "def categorize_query(query):\n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        if any(keyword in query.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "## Function to get response based on category\n",
    "RESPONSES = {\n",
    "    \"Billing\": \"For billing questions, please check your account settings or contact our billing support team.\",\n",
    "    \"Technical Support\": \"For technical support, please try to restart your device or describe your issue in detail and our technical support team will assist you.\",\n",
    "    \"General Inquiry\": \"For general inquiries, please visit our FAQ page or contact our general support team. Our business hours are 9 AM to 5 PM, Monday to Friday.\",\n",
    "    \"Unknown\": \"I'm sorry, I couldn't categorize your query. Please provide more details as I connect you with a support agent.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3270ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mistral_client\n",
    "MISTRAL_API, mistral_client = load_environment_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c813e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate response\n",
    "def generate_response(query, category):\n",
    "    if category in RESPONSES:\n",
    "        return RESPONSES[category]\n",
    "    \n",
    "    # If category is unknown, use RAG to generate a response\n",
    "    relevant_chunks = mistral_client.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    \n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    response = mistral_client.chat.completions.create(\n",
    "        model=\"mistral-7b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful customer support assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a20ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run customer support\n",
    "def Query(user_query):\n",
    "    # Load environment variables and initialize Mistral client\n",
    "    MISTRAL_API, client = load_environment_variables()\n",
    "    \n",
    "    # Initialize chatbot state\n",
    "    state = ChatbotState()\n",
    "    state.user_query = user_query\n",
    "    \n",
    "    # Categorize the user query\n",
    "    state.category = categorize_query(state.user_query)\n",
    "    \n",
    "    # Generate response based on category\n",
    "    RESPONSES = generate_response(state.user_query, state.category)\n",
    "    \n",
    "\n",
    "    return RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3932787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Using cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/sentence-transformers/\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "653bdf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Your invoice is available in your account settings.\n",
      "2. If you were incorrectly charged, please contact support for a refund.\n",
      "3. You can make a payment using credit card or PayPal.\n",
      "4. For technical issues, try restarting your device.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Your invoice is available in your account settings.\",\n",
    "    \"You can make a payment using credit card or PayPal.\",\n",
    "    \"If you were incorrectly charged, please contact support for a refund.\",\n",
    "    \"For technical issues, try restarting your device.\",\n",
    "    \"If the app crashes, please report the bug to our technical team.\",\n",
    "    \"For general inquiries, visit our FAQ page or contact support.\",\n",
    "    \"Our business hours are 9 AM to 5 PM, Monday to Friday.\"\n",
    "]\n",
    "\n",
    "# Wrap documents in LangChain Document objects\n",
    "docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Create embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"I have an issue with my invoice and payment.\"\n",
    "results = vector_store.similarity_search(query)\n",
    "\n",
    "# Print results\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"{i+1}. {res.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a971ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: For billing questions, please check your account settings or contact our billing support team.\n"
     ]
    }
   ],
   "source": [
    "## Ask a question\n",
    "user_query = \"I have an issue with my invoice and payment.\"\n",
    "response = Query(user_query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc84c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user interface\n",
    "from zipfile import PyZipFile\n",
    "import gradio as gr\n",
    "\n",
    "def respond_to_query(user_query):\n",
    "    response = Query(user_query)\n",
    "    return response\n",
    "\n",
    "iface = gr.Interface(fn=respond_to_query, inputs=\"text\", outputs=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84c40b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(user_query):\n",
    "    # Retrieve relevant documents from FAISS\n",
    "    results = vector_store.similarity_search(user_query, k=3)\n",
    "    \n",
    "    # Combine retrieved content\n",
    "    context = \"\\n\".join([doc.page_content for doc in results])\n",
    "    \n",
    "    # Generate a response using Mistral or another LLM\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    response = mistral_client.chat(prompt=prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32556\\2074318980.py\", line 10, in rag_query\n",
      "    response = mistral_client.chat(prompt=prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'Chat' object is not callable\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32556\\2074318980.py\", line 10, in rag_query\n",
      "    response = mistral_client.chat(prompt=prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'Chat' object is not callable\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32556\\2074318980.py\", line 10, in rag_query\n",
      "    response = mistral_client.chat(prompt=prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'Chat' object is not callable\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 745, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32556\\2074318980.py\", line 10, in rag_query\n",
      "    response = mistral_client.chat(prompt=prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'Chat' object is not callable\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "demo = gr.Interface(\n",
    "    fn=rag_query,\n",
    "    inputs=gr.Textbox(lines=5, placeholder=\"Ask a question about customer support...\"),\n",
    "    outputs=gr.Textbox(lines=10),\n",
    "    title=\"Customer Support Chatbot\"\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
